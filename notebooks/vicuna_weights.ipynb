{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vicuna Weights\n",
    "\n",
    "FastChat provides [Vicuna](https://vicuna.lmsys.org/) weights as delta weights to comply with the LLaMA model license. You can add our delta to the original LLaMA weights to obtain the Vicuna weights. The following scripts can be used to get Vicuna weights by applying our delta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get the original LLaMA weights in the huggingface format by following the code block or instructions [here](https://huggingface.co/docs/transformers/main/model_doc/llama)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import transformers\n",
    "\n",
    "# Llama model directory\n",
    "Llama_model_dir = \"path/to/LLaMA\"\n",
    "\n",
    "# Huggingface model directory\n",
    "Huggingface_model_dir = \"huggingface_LLaMA\"\n",
    "\n",
    "# Transformers library directory\n",
    "transformers_dir = os.path.dirname(transformers.__file__)\n",
    "\n",
    "model_size = \"7B\"\n",
    "\n",
    "# Create the arguments required to run the command\n",
    "cmd = f\"python3 {os.path.join(transformers_dir, 'models', 'llama', 'convert_llama_weights_to_hf.py')} \\\n",
    "    --input_dir {Llama_model_dir} --model_size {model_size} --output_dir {Huggingface_model_dir}/{model_size}\"\n",
    "\n",
    "# Run the command\n",
    "os.system(cmd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the following scripts to get Vicuna weights by applying our delta. They will automatically download delta weights from our Hugging Face [account](https://huggingface.co/lmsys).\n",
    "\n",
    "**NOTE**:\n",
    "Our released weights are only compatible with the latest main branch of huggingface/transformers.\n",
    "We install the correct version of transformers when fastchat is installed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Vicuna-7B\n",
    "This conversion command needs around 30 GB of CPU RAM.\n",
    "If you do not have enough memory, you can create a large swap file that allows the operating system to automatically utilize the disk as virtual memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "delta_path = \"lmsys/vicuna-7b-delta-v1.1\"\n",
    "vicuna_model_dir = \"vicuna_LLaMA\"\n",
    "\n",
    "# Create the arguments required to run the command\n",
    "cmd = f\"python3 -m fastchat.model.apply_delta \\\n",
    "    --base {Huggingface_model_dir}/7B \\\n",
    "    --target {vicuna_model_dir}/7B \\\n",
    "    --delta {delta_path}\"\n",
    "\n",
    "# Run the command\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vicuna-13B\n",
    "This conversion command needs around 60 GB of CPU RAM.\n",
    "If you do not have enough memory, you can create a large swap file that allows the operating system to automatically utilize the disk as virtual memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "delta_path = \"lmsys/vicuna-13b-delta-v1.1\"\n",
    "vicuna_model_dir = \"vicuna_LLaMA\"\n",
    "\n",
    "# Create the arguments required to run the command\n",
    "cmd = f\"python3 -m fastchat.model.apply_delta \\\n",
    "    --base {Huggingface_model_dir}/13B \\\n",
    "    --target {vicuna_model_dir}/13B \\\n",
    "    --delta {delta_path}\"\n",
    "\n",
    "# Run the command\n",
    "os.system(cmd)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
