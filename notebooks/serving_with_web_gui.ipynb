{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "*  Install the FastChat package by following the instructions in the parent directory's [README.md](../README.md) file or follow the [tutorial.ipynb](tutorial.ipynb) notebook's Installation section.\n",
    "*  Get vicuna weights for a chatbot by following the instructions in the [vicuna_weights.ipynb](vicuna_weights.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving with Web GUI\n",
    "\n",
    "To serve using the web UI, you need three main components: web servers that interface with users, model workers that host one or more models, and a controller to coordinate the webserver and model workers. Here are the commands to follow in your terminal:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the controller\n",
    "The controller is responsible for coordinating the webserver and model workers. It needs to be launched first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess # we need this to run the controller in a separate process in the jupyter notebook\n",
    "subprocess.Popen([\"python3\", \"-m\", \"fastchat.serve.controller\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the model worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vicuna_weight_path = # path/to/vicuna/weights\n",
    "device = \"cuda\" # or \"cpu\" / \"mps\"\n",
    "subprocess.Popen([\"python3\", \"-m\", \"fastchat.serve.model_worker\",\"--device\",device, \"--model-path\", vicuna_weight_path])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have enough memory, you can enable 8-bit compression by adding `--load-8bit` to commands above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until the process finishes loading the model and you see **\"Uvicorn running on ...\"**. You can launch multiple model workers to serve multiple models concurrently. The model worker will connect to the controller automatically.\n",
    "\n",
    "To ensure that your model worker is connected to your controller properly, send a test message using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m fastchat.serve.test_message --model-name vicuna-7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the Gradio web server\n",
    "This is the user interface that users will interact with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.Popen([\"python3\", \"-m\", \"fastchat.serve.gradio_web_server\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kill the processes\n",
    "To kill the processes, you can use the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!kill -9 $(lsof -t -i:21001) # kill the controller\n",
    "!kill -9 $(lsof -t -i:21002) # kill the worker\n",
    "!kill -9 $(lsof -t -i:7860) # kill the gradio server\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
