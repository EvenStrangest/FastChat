{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "* Install the FastChat package by following the instructions in the parent directory's [README.md](../README.md) file or follow the [tutorial.ipynb](tutorial.ipynb) notebook's Installation section.\n",
    "* Get vicuna weights for a chatbot by following the instructions in the [vicuna_weights.ipynb](vicuna_weights.ipynb) notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with Command Line Interface\n",
    "The FastChat CLI provides a command-line interface for inference. You can specify different options to configure the inference process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single GPU\n",
    "The command below requires around 28GB of GPU memory for Vicuna-13B and 14GB of GPU memory for Vicuna-7B.\n",
    "See the \"No Enough Memory\" section below if you do not have enough memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vicuna_weight_path= # /path/to/vicuna/weights\n",
    "!python3 -m fastchat.serve.cli --model-path {vicuna_weight_path} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple GPUs\n",
    "If you do not have enough GPU memory, you can use model parallelism to aggregate memory from multiple GPUs on the same machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vicuna_weight_path= # /path/to/vicuna/weights\n",
    "!python3 -m fastchat.serve.cli  --num-gpus 2 --model-path {vicuna_weight_path}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Enough Memory or Other Platforms\n",
    "If you do not have enough memory, you can enable 8-bit compression by adding `--load-8bit` to commands above.\n",
    "This can reduce memory usage by around half with slightly degraded model quality.\n",
    "It is compatible with the CPU, GPU, and Metal backend.\n",
    "Vicuna-13B with 8-bit compression can run on a single NVIDIA 3090/4080/V100(16GB) GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vicuna_weight_path = # /path/to/vicuna/weights\n",
    "!python3 -m fastchat.serve.cli  --load-8bit --model-path {vicuna_weight_path} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving with Web GUI\n",
    "\n",
    "To serve using the web UI, you need three main components: web servers that interface with users, model workers that host one or more models, and a controller to coordinate the webserver and model workers. Here are the commands to follow in your terminal:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the controller\n",
    "The controller is responsible for coordinating the webserver and model workers. It needs to be launched first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess # we need this to run the controller in a separate process in the jupyter notebook\n",
    "subprocess.Popen([\"python3\", \"-m\", \"fastchat.serve.controller\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the model worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vicuna_weight_path = # /path/to/vicuna/weights\n",
    "device = \"cuda\" # or \"cpu\" / \"mps\"\n",
    "subprocess.Popen([\"python3\", \"-m\", \"fastchat.serve.model_worker\",\"--device\",device, \"--model-path\", vicuna_weight_path])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have enough memory, you can enable 8-bit compression by adding `--load-8bit` to commands above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until the process finishes loading the model and you see **\"Uvicorn running on ...\"**. You can launch multiple model workers to serve multiple models concurrently. The model worker will connect to the controller automatically.\n",
    "\n",
    "To ensure that your model worker is connected to your controller properly, send a test message using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m fastchat.serve.test_message --model-name vicuna-7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the Gradio web server\n",
    "This is the user interface that users will interact with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.Popen([\"python3\", \"-m\", \"fastchat.serve.gradio_web_server\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kill the processes\n",
    "To kill the processes, you can use the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!kill -9 $(lsof -t -i:21001) # kill the controller\n",
    "!kill -9 $(lsof -t -i:21002) # kill the worker\n",
    "!kill -9 $(lsof -t -i:7860) # kill the gradio server\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
