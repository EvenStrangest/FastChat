{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastChat Jupyter Notebook Guide\n",
    "FastChat is an open-source platform for training, serving, and evaluating large language model-based chatbots. In this guide, we will cover the installation process and the main features of FastChat.\n",
    "- [Installation](#Installation)\n",
    "- [Vicuna Weights](#Vicuna-Weights)\n",
    "- [Inference with Command Line Interface](#Inference-with-Command-Line-Interface)\n",
    "- [Serving with Web GUI](#Serving-with-Web-GUI)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Before starting to use FastChat, it needs to be installed on your system. There are two ways to install FastChat, either through pip or by cloning the repository and installing it from source."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: With pip\n",
    "The following commands can be used to install FastChat and the latest main branch of transformers with pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip3 install fschat\n",
    "!pip3 install git+https://github.com/huggingface/transformers\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: From source\n",
    "To install FastChat from source, first, clone the repository and navigate to the FastChat folder:\n",
    "\n",
    "**If you are running on Mac, you need to install Rust and CMake before proceeding:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !brew install rust cmake"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, install FastChat by running the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/lm-sys/FastChat.git\n",
    "# !cd FastChat\n",
    "\n",
    "# !pip3 install --upgrade pip\n",
    "# !pip3 install -e ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vicuna Weights\n",
    "\n",
    "FastChat provides [Vicuna](https://vicuna.lmsys.org/) weights as delta weights to comply with the LLaMA model license. You can add our delta to the original LLaMA weights to obtain the Vicuna weights. The following scripts can be used to get Vicuna weights by applying our delta:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get the original LLaMA weights in the huggingface format by following the code block or instructions [here](https://huggingface.co/docs/transformers/main/model_doc/llama)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import transformers\n",
    "\n",
    "# Llama model directory\n",
    "Llama_model_dir = \"LLaMA\"\n",
    "\n",
    "# Huggingface model directory\n",
    "Huggingface_model_dir = \"huggingface_LLaMA\"\n",
    "\n",
    "# Transformers library directory\n",
    "transformers_dir = os.path.dirname(transformers.__file__)\n",
    "\n",
    "model_size = \"7B\"\n",
    "\n",
    "# Create the arguments required to run the command\n",
    "cmd = f\"python3 {os.path.join(transformers_dir, 'models', 'llama', 'convert_llama_weights_to_hf.py')} \\\n",
    "    --input_dir {Llama_model_dir} --model_size {model_size} --output_dir {Huggingface_model_dir}/{model_size}\"\n",
    "\n",
    "# Run the command\n",
    "os.system(cmd)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the following scripts to get Vicuna weights by applying our delta. They will automatically download delta weights from our Hugging Face [account](https://huggingface.co/lmsys).\n",
    "\n",
    "**NOTE**:\n",
    "Our released weights are only compatible with the latest main branch of huggingface/transformers.\n",
    "We install the correct version of transformers when fastchat is installed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Vicuna-7B\n",
    "This conversion command needs around 30 GB of CPU RAM.\n",
    "If you do not have enough memory, you can create a large swap file that allows the operating system to automatically utilize the disk as virtual memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "delta_path = \"lmsys/vicuna-7b-delta-v1.1\"\n",
    "vicuna_model_dir = \"vicuna_LLaMA\"\n",
    "\n",
    "# Create the arguments required to run the command\n",
    "cmd = f\"python3 -m fastchat.model.apply_delta \\\n",
    "    --base {Huggingface_model_dir}/7B \\\n",
    "    --target {vicuna_model_dir}/7B \\\n",
    "    --delta {delta_path}\"\n",
    "\n",
    "# Run the command\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vicuna-13B\n",
    "This conversion command needs around 60 GB of CPU RAM.\n",
    "If you do not have enough memory, you can create a large swap file that allows the operating system to automatically utilize the disk as virtual memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "delta_path = \"lmsys/vicuna-13b-delta-v1.1\"\n",
    "vicuna_model_dir = \"vicuna_LLaMA\"\n",
    "\n",
    "# Create the arguments required to run the command\n",
    "cmd = f\"python3 -m fastchat.model.apply_delta \\\n",
    "    --base {Huggingface_model_dir}/13B \\\n",
    "    --target {vicuna_model_dir}/13B \\\n",
    "    --delta {delta_path}\"\n",
    "\n",
    "# Run the command\n",
    "os.system(cmd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with Command Line Interface\n",
    "The FastChat CLI provides a command-line interface for inference. You can specify different options to configure the inference process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single GPU\n",
    "The command below requires around 28GB of GPU memory for Vicuna-13B and 14GB of GPU memory for Vicuna-7B.\n",
    "See the \"No Enough Memory\" section below if you do not have enough memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vicuna_weight_path= vicuna_model_dir + \"/7B\"\n",
    "!python3 -m fastchat.serve.cli --model-path {vicuna_weight_path} # or /path/to/vicuna/weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple GPUs\n",
    "If you do not have enough GPU memory, you can use model parallelism to aggregate memory from multiple GPUs on the same machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vicuna_weight_path= f\"{vicuna_model_dir}/7B\"\n",
    "!python3 -m fastchat.serve.cli  --num-gpus 2 --model-path {vicuna_weight_path} # or /path/to/vicuna/weights\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU Only\n",
    "Use `--device cpu` to use CPU only and does not require GPU. It requires around 60GB of CPU memory for Vicuna-13B and around 30GB of CPU memory for Vicuna-7B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vicuna_weight_path= f\"{vicuna_model_dir}/7B\"\n",
    "!python3 -m fastchat.serve.cli  --device cpu --model-path {vicuna_weight_path} # or /path/to/vicuna/weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metal Backend (Mac Computers with Apple Silicon or AMD GPUs)\n",
    "Use `--device mps` to enable GPU acceleration on Mac computers (requires torch >= 2.0).\n",
    "Use `--load-8bit` to turn on 8-bit compression. Vicuna-7B can run on a 32GB M1 Macbook with 1 - 2 words / second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vicuna_weight_path= f\"{vicuna_model_dir}/7B\"\n",
    "!python3 -m fastchat.serve.cli  --device mps --load-8bit --model-path {vicuna_weight_path} # or /path/to/vicuna/weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Enough Memory or Other Platforms\n",
    "If you do not have enough memory, you can enable 8-bit compression by adding `--load-8bit` to commands above.\n",
    "This can reduce memory usage by around half with slightly degraded model quality.\n",
    "It is compatible with the CPU, GPU, and Metal backend.\n",
    "Vicuna-13B with 8-bit compression can run on a single NVIDIA 3090/4080/V100(16GB) GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vicuna_weight_path = f\"{vicuna_model_dir}/7B\"\n",
    "!python3 -m fastchat.serve.cli  --load-8bit --model-path {vicuna_weight_path} # or /path/to/vicuna/weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving with Web GUI\n",
    "\n",
    "To serve using the web UI, you need three main components: web servers that interface with users, model workers that host one or more models, and a controller to coordinate the webserver and model workers. Here are the commands to follow in your terminal:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the controller\n",
    "The controller is responsible for coordinating the webserver and model workers. It needs to be launched first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess # we need this to run the controller in a separate process in the jupyter notebook\n",
    "subprocess.Popen([\"python3\", \"-m\", \"fastchat.serve.controller\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the model worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vicuna_weight_path = f\"{vicuna_model_dir}/7B\"\n",
    "device = \"cuda\" # or \"cpu\" / \"mps\"\n",
    "subprocess.Popen([\"python3\", \"-m\", \"fastchat.serve.model_worker\",\"--device\",device, \"--model-path\", vicuna_weight_path])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until the process finishes loading the model and you see **\"Uvicorn running on ...\"**. You can launch multiple model workers to serve multiple models concurrently. The model worker will connect to the controller automatically.\n",
    "\n",
    "To ensure that your model worker is connected to your controller properly, send a test message using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m fastchat.serve.test_message --model-name vicuna-7b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the Gradio web server\n",
    "This is the user interface that users will interact with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.Popen([\"python3\", \"-m\", \"fastchat.serve.gradio_web_server\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kill the processes\n",
    "To kill the processes, you can use the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!kill -9 $(lsof -t -i:21001) # kill the controller\n",
    "!kill -9 $(lsof -t -i:21002) # kill the worker\n",
    "!kill -9 $(lsof -t -i:7860) # kill the gradio server\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
